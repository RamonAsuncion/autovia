{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3a9222",
   "metadata": {},
   "source": [
    "# dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a028037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090f356d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m model_checkpoints_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tensorboard_logger_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tb_logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCityscapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msemantic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cityscapes.py:155\u001b[0m, in \u001b[0;36mCityscapes.__init__\u001b[0;34m(self, root, split, mode, target_type, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m    153\u001b[0m         extract_archive(from_path\u001b[38;5;241m=\u001b[39mtarget_dir_zip, to_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or incomplete. Please make sure all required folders for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are inside the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir):\n\u001b[1;32m    161\u001b[0m     img_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, city)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory"
     ]
    }
   ],
   "source": [
    "data_dir = \"~/data/cityscapes/\"\n",
    "model_weights_dir = \"./weights/\"\n",
    "model_checkpoints_dir = \"./checkpoints/\"\n",
    "tensorboard_logger_dir = \"./tb_logs/\"\n",
    "\n",
    "\n",
    "dataset = Cityscapes(data_dir, split='train', mode='fine',\n",
    "                      target_type='semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4824d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,figsize=(12,8))\n",
    "ax[0].imshow(dataset[0][0])\n",
    "ax[1].imshow(dataset[0][1],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce34a07",
   "metadata": {},
   "source": [
    "# some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15070a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label understanding\n",
    "#https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\n",
    "\n",
    "#function credits: https://github.com/meetps/pytorch-semseg/tree/master/ptsemseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_index=255\n",
    "void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "valid_classes = [ignore_index,7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light', \\\n",
    "               'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', \\\n",
    "               'train', 'motorcycle', 'bicycle']\n",
    "#why i choose 20 classes\n",
    "#https://stackoverflow.com/a/64242989\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes=len(valid_classes)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed178bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [   [  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32],\n",
    "    ]\n",
    "\n",
    "label_colours = dict(zip(range(n_classes), colors))\n",
    "label_colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_segmap(mask):\n",
    "    #remove unwanted classes and rectify the labels of wanted classes\n",
    "    for _voidc in void_classes:\n",
    "        mask[mask == _voidc] = ignore_index\n",
    "    for _validc in valid_classes:\n",
    "        mask[mask == _validc] = class_map[_validc]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb81e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(temp):\n",
    "    #convert gray scale to color\n",
    "    temp=temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46436746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "transform=A.Compose(\n",
    "[\n",
    "    A.Resize(256, 512),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "class MyClass(Cityscapes):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "\n",
    "        targets: Any = []\n",
    "        for i, t in enumerate(self.target_type):\n",
    "            if t == 'polygon':\n",
    "                target = self._load_json(self.targets[index][i])\n",
    "            else:\n",
    "                target = Image.open(self.targets[index][i])\n",
    "            targets.append(target)\n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed=transform(image=np.array(image), mask=np.array(target))            \n",
    "        return transformed['image'],transformed['mask']\n",
    "    #torch.unsqueeze(transformed['mask'],0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MyClass(data_dir, split='val', mode='fine',\n",
    "                     target_type='semantic',transforms=transform)\n",
    "img,seg= dataset[20]\n",
    "print(img.shape,seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a161c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(16,8))\n",
    "ax[0].imshow(img.permute(1, 2, 0))\n",
    "ax[1].imshow(seg,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class labels before label correction\n",
    "print(torch.unique(seg))\n",
    "print(len(torch.unique(seg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class labels after label correction\n",
    "res=encode_segmap(seg.clone())\n",
    "print(res.shape)\n",
    "print(torch.unique(res))\n",
    "print(len(torch.unique(res)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let do coloring\n",
    "res1=decode_segmap(res.clone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6176dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,figsize=(12,10))  \n",
    "ax[0].imshow(res,cmap='gray')\n",
    "ax[1].imshow(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b337c",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "import segmentation_models_pytorch as smp\n",
    "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import multiprocessing\n",
    "import torchmetrics\n",
    "import torch\n",
    "\n",
    "# for reproducibility\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce873bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModel(LightningModule):\n",
    "  def __init__(self):\n",
    "    super(OurModel,self).__init__()\n",
    "    #architecute\n",
    "    self.layer = smp.Unet(\n",
    "                encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "                in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                classes=n_classes,                      # model output channels (number of classes in your dataset)\n",
    "            )\n",
    "\n",
    "    #parameters\n",
    "    self.lr=1e-3\n",
    "    self.batch_size=4\n",
    "    self.numworker=multiprocessing.cpu_count()//4\n",
    "\n",
    "    self.criterion= smp.losses.DiceLoss(mode='multiclass')\n",
    "    self.metrics = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=n_classes)\n",
    "    \n",
    "    self.train_class = MyClass(data_dir, split='train', mode='fine',\n",
    "                      target_type='semantic',transforms=transform)\n",
    "    self.val_class = MyClass(data_dir, split='val', mode='fine',\n",
    "                      target_type='semantic',transforms=transform)\n",
    "    \n",
    "    \n",
    "  def process(self,image,segment):\n",
    "    out=self(image)\n",
    "    segment=encode_segmap(segment)\n",
    "    loss=self.criterion(out,segment.long())\n",
    "    jaccard=self.metrics(out,segment)\n",
    "    return loss,jaccard\n",
    "    \n",
    "  def forward(self,x):\n",
    "    return self.layer(x)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    opt=torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    return opt\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_class, batch_size=self.batch_size, \n",
    "                      shuffle=True,num_workers=self.numworker,pin_memory=True)\n",
    "\n",
    "  def training_step(self,batch,batch_idx):\n",
    "    image,segment=batch\n",
    "    loss,jaccard=self.process(image,segment)\n",
    "    self.log('train_loss', loss,on_step=False, on_epoch=True,prog_bar=True)\n",
    "    self.log('train_jaccard', jaccard,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    return loss\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_class, batch_size=self.batch_size, \n",
    "                      shuffle=False,num_workers=self.numworker,pin_memory=True)\n",
    "    \n",
    "  def validation_step(self,batch,batch_idx):\n",
    "    image,segment=batch\n",
    "    loss,jaccard=self.process(image,segment)\n",
    "    self.log('val_loss', loss,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    self.log('val_jaccard', jaccard,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71013f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = True\n",
    "\n",
    "model = OurModel()\n",
    "\n",
    "if resume_training: \n",
    "    model.load_state_dict(torch.load(model_weights_dir+'model.pth'))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                        dirpath=model_checkpoints_dir,\n",
    "                                        filename='checkpoint_file',\n",
    "                                        save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(max_epochs=200, auto_lr_find=False, auto_scale_batch_size=False,\n",
    "#                   gpus=-1,precision=16,\n",
    "#                   callbacks=[checkpoint_callback],\n",
    "#                  )\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=tensorboard_logger_dir, name=\"semantic_segmentation_cityscapes\")\n",
    "\n",
    "trainer = Trainer(max_epochs = 5,\n",
    "                    callbacks=[checkpoint_callback],\n",
    "                    logger = logger,\n",
    "                    deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb881f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)\n",
    "torch.save(model.state_dict(), model_weights_dir+'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_weights_dir+'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = MyClass(data_dir, split='val', mode='fine',\n",
    "                     target_type='semantic',transforms=transform)\n",
    "test_loader=DataLoader(test_class, batch_size=12, \n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e60336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img,seg=batch\n",
    "        output=model(img.cuda())\n",
    "        break\n",
    "print(img.shape,seg.shape,output.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample=6\n",
    "invimg=inv_normalize(img[sample])\n",
    "outputx=output.detach().cpu()[sample]\n",
    "encoded_mask=encode_segmap(seg[sample].clone()) #(256, 512)\n",
    "decoded_mask=decode_segmap(encoded_mask.clone())  #(256, 512)\n",
    "decoded_ouput=decode_segmap(torch.argmax(outputx,0))\n",
    "fig,ax=plt.subplots(ncols=3,figsize=(16,50),facecolor='white')  \n",
    "ax[0].imshow(np.moveaxis(invimg.numpy(),0,2)) #(3,256, 512)\n",
    "#ax[1].imshow(encoded_mask,cmap='gray') #(256, 512)\n",
    "ax[1].imshow(decoded_mask) #(256, 512, 3)\n",
    "ax[2].imshow(decoded_ouput) #(256, 512, 3)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[2].axis('off')\n",
    "ax[0].set_title('Input Image')\n",
    "ax[1].set_title('Ground mask')\n",
    "ax[2].set_title('Predicted mask')\n",
    "# plt.savefig('result.png',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
